{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13530,"status":"ok","timestamp":1698136487444,"user":{"displayName":"daeyoung kim","userId":"16329661665482311022"},"user_tz":-540},"id":"TpxBtwprTNfG","outputId":"10010a13-0ecf-45de-cb34-9d6f758c474a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59141,"status":"ok","timestamp":1698136546581,"user":{"displayName":"daeyoung kim","userId":"16329661665482311022"},"user_tz":-540},"id":"0XCI3tBsNFTR","outputId":"8c436b6e-42bb-47fc-ccef-3abba68ca42a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting lightning@ git+https://github.com/Lightning-AI/lightning@master (from -r drive/MyDrive/requirements/chatbot.txt (line 2))\n","  Cloning https://github.com/Lightning-AI/lightning (to revision master) to /tmp/pip-install-ao4row8x/lightning_531e5e9e3b28438d992e880543273f20\n","  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-ao4row8x/lightning_531e5e9e3b28438d992e880543273f20\n","  Resolved https://github.com/Lightning-AI/lightning to commit c5a731c3cdf1bc29cb9323511dda2528c3ab5835\n","  Running command git submodule update --init --recursive -q\n","  Encountered 22 file(s) that should have been pointers, but weren't:\n","        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n","        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n","        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n","        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n","        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n","        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n","        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n","        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n","        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n","        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n","        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n","        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n","        .notebooks/flash_tutorials/image_classification.ipynb\n","        .notebooks/flash_tutorials/tabular_classification.ipynb\n","        .notebooks/flash_tutorials/text_classification.ipynb\n","        .notebooks/lightning_examples/cifar10-baseline.ipynb\n","        .notebooks/lightning_examples/datamodules.ipynb\n","        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n","        .notebooks/lightning_examples/warp-drive.ipynb\n","        .notebooks/templates/img-classify.ipynb\n","        .notebooks/templates/simple.ipynb\n","        .notebooks/templates/titanic.ipynb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r drive/MyDrive/requirements/chatbot.txt (line 1)) (2.1.0+cu118)\n","Collecting sentencepiece (from -r drive/MyDrive/requirements/chatbot.txt (line 3))\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r drive/MyDrive/requirements/chatbot.txt (line 4)) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r drive/MyDrive/requirements/chatbot.txt (line 5)) (1.23.5)\n","Collecting jsonargparse[signatures] (from -r drive/MyDrive/requirements/chatbot.txt (line 6))\n","  Downloading jsonargparse-4.26.1-py3-none-any.whl (187 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes (from -r drive/MyDrive/requirements/chatbot.txt (line 7))\n","  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from -r drive/MyDrive/requirements/chatbot.txt (line 8))\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting zstandard (from -r drive/MyDrive/requirements/chatbot.txt (line 9))\n","  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi (from -r drive/MyDrive/requirements/chatbot.txt (line 10))\n","  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn (from -r drive/MyDrive/requirements/chatbot.txt (line 11))\n","  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyngrok (from -r drive/MyDrive/requirements/chatbot.txt (line 12))\n","  Downloading pyngrok-7.0.0.tar.gz (718 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.7/718.7 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (2.1.0)\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r drive/MyDrive/requirements/chatbot.txt (line 2)) (6.0.1)\n","Collecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r drive/MyDrive/requirements/chatbot.txt (line 2))\n","  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r drive/MyDrive/requirements/chatbot.txt (line 2)) (23.2)\n","Collecting torchmetrics<3.0,>=0.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r drive/MyDrive/requirements/chatbot.txt (line 2))\n","  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r drive/MyDrive/requirements/chatbot.txt (line 2))\n","  Downloading pytorch_lightning-2.1.0-py3-none-any.whl (774 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.6/774.6 kB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r drive/MyDrive/requirements/chatbot.txt (line 6))\n","  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n","Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r drive/MyDrive/requirements/chatbot.txt (line 6))\n","  Downloading typeshed_client-2.4.0-py3-none-any.whl (594 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8))\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (3.4.1)\n","Collecting multiprocess (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8))\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (3.8.6)\n","Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8))\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r drive/MyDrive/requirements/chatbot.txt (line 10)) (3.7.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r drive/MyDrive/requirements/chatbot.txt (line 10)) (1.10.13)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi->-r drive/MyDrive/requirements/chatbot.txt (line 10))\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions (from torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1))\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r drive/MyDrive/requirements/chatbot.txt (line 11)) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn->-r drive/MyDrive/requirements/chatbot.txt (line 11))\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r drive/MyDrive/requirements/chatbot.txt (line 10)) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r drive/MyDrive/requirements/chatbot.txt (line 10)) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r drive/MyDrive/requirements/chatbot.txt (line 10)) (1.1.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (1.3.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (2023.7.22)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]->-r drive/MyDrive/requirements/chatbot.txt (line 6)) (6.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (2023.3.post1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->-r drive/MyDrive/requirements/chatbot.txt (line 1)) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r drive/MyDrive/requirements/chatbot.txt (line 8)) (1.16.0)\n","Building wheels for collected packages: lightning, pyngrok\n","  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lightning: filename=lightning-2.2.0.dev0-py3-none-any.whl size=2002544 sha256=a1dd76786f321081cf5659d805f48bab96416b84f064648ae25814f0b480a548\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m9o7efzl/wheels/b9/92/07/634ec381ab7d682d3afdcf943d0a6604881441ff2d6c409103\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-7.0.0-py3-none-any.whl size=21129 sha256=286a7d90992540337689d863a4dcd237bbb1457893cd47f1ab8660a7b4c336a4\n","  Stored in directory: /root/.cache/pip/wheels/60/29/7b/f64332aa7e5e88fbd56d4002185ae22dcdc83b35b3d1c2cbf5\n","Successfully built lightning pyngrok\n","Installing collected packages: sentencepiece, bitsandbytes, zstandard, typing-extensions, typeshed-client, pyngrok, jsonargparse, h11, docstring-parser, dill, uvicorn, starlette, multiprocess, lightning-utilities, huggingface-hub, torchmetrics, fastapi, pytorch-lightning, datasets, lightning\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bitsandbytes-0.41.1 datasets-2.14.6 dill-0.3.7 docstring-parser-0.15 fastapi-0.104.0 h11-0.14.0 huggingface-hub-0.18.0 jsonargparse-4.26.1 lightning-2.2.0.dev0 lightning-utilities-0.9.0 multiprocess-0.70.15 pyngrok-7.0.0 pytorch-lightning-2.1.0 sentencepiece-0.1.99 starlette-0.27.0 torchmetrics-1.2.0 typeshed-client-2.4.0 typing-extensions-4.8.0 uvicorn-0.23.2 zstandard-0.21.0\n"]}],"source":["# requirements\n","\n","!pip install -r drive/MyDrive/requirements.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1698136546581,"user":{"displayName":"daeyoung kim","userId":"16329661665482311022"},"user_tz":-540},"id":"cUGd-utKC4JO","outputId":"f7e77ba9-610b-40c3-bc86-10e2439104e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ChatBot\n"]}],"source":["# change dir\n","\n","%cd drive/MyDrive/ChatBot"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3118353,"status":"ok","timestamp":1698139664924,"user":{"displayName":"daeyoung kim","userId":"16329661665482311022"},"user_tz":-540},"id":"oaIQDkRFWXJR","outputId":"78991c1d-841e-4893-db60-0a4ab02d6e30"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stderr","output_type":"stream","text":["WARNING:pyngrok.process.ngrok:t=2023-10-24T08:39:44+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n","INFO:     Started server process [1267]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"]},{"name":"stdout","output_type":"stream","text":["FastAPI server is available at: NgrokTunnel: \"https://c578-34-143-163-154.ngrok.io\" -> \"http://localhost:8000\"\n","user input: 두통이 너무 심해요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 복통이 너무 심해요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 두통이 너무 심해요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 배가 너무 아파요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 복통이 너무 심해요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 복통이 너무 심해요\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 복통이 너무 심해요\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 배가 너무 아파요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 배가 너무 아파요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n","user input: 두통이 너무 심해요.\n","INFO:     210.101.130.131:0 - \"POST /post_data HTTP/1.1\" 200 OK\n","INFO:     210.101.130.131:0 - \"GET /get_data HTTP/1.1\" 200 OK\n"]},{"name":"stderr","output_type":"stream","text":["INFO:     Shutting down\n","INFO:     Waiting for application shutdown.\n","INFO:     Application shutdown complete.\n","INFO:     Finished server process [1267]\n"]}],"source":["# external path\n","\n","from pathlib import Path\n","import lightning as L\n","import torch\n","\n","from lit_llama import LLaMA, Tokenizer\n","from lit_llama.utils import EmptyInitOnDevice\n","\n","from fastapi import FastAPI\n","from fastapi.responses import JSONResponse\n","from pydantic import BaseModel\n","\n","import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n","\n","class ChatBot:\n","    def __init__(self, model, tokenizer, fabric):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.fabric = fabric\n","        self.history = []\n","\n","    def generate_prompt(self, example):\n","        if example[\"input\"]:\n","            return (\n","                \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n","                \"요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","                f\"### 명령어:\\n{example['instruction']}\\n\\n### 입력:\\n{example['input']}\\n\\n### 응답:\"\n","            )\n","        return (\n","            \"환자가 의사에게 아픈 곳에 대해 문의합니다.\\n\\n\"\n","            \"환자의 문의 내용에 대해 답변하세요. 환자의 질병을 진단하고, 가능하면 처방을 하세요. \\n\\n\"\n","            f\"### 문의:\\n{example['instruction']}\\n\\n### 응답:\"\n","    )\n","\n","    # default generation\n","    @torch.no_grad()\n","    def generate(\n","        self,\n","        idx,\n","        max_new_tokens,\n","        max_seq_length=None,\n","        temperature=0.8,\n","        top_k=None,\n","        eos_id=None,\n","        repetition_penalty=1.1,\n","        early_stopping=True,\n","    ):\n","        T = idx.size(0)\n","        T_new = T + max_new_tokens\n","        if max_seq_length is None:\n","            max_seq_length = min(T_new, self.model.config.block_size)\n","\n","        device, dtype = idx.device, idx.dtype\n","        # create an empty tensor of the expected final shape and fill in the current tokens\n","        empty = torch.empty(T_new, dtype=dtype, device=device)\n","        empty[:T] = idx\n","        idx = empty\n","        input_pos = torch.arange(0, T, device=device)\n","\n","        if idx.device.type == \"xla\":\n","            import torch_xla.core.xla_model as xm\n","\n","            xm.mark_step()\n","\n","        # generate max_new_tokens tokens\n","        for _ in range(max_new_tokens):\n","            x = idx.index_select(0, input_pos).view(1, -1)\n","\n","            # forward\n","            logits = self.model(x, max_seq_length, input_pos)\n","            logits = logits[0, -1] / temperature\n","\n","            # optionally crop the logits to only the top k options\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n","\n","            probs = torch.nn.functional.softmax(logits, dim=-1)\n","            idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n","\n","            # advance\n","            input_pos = input_pos[-1:] + 1\n","\n","            if idx.device.type == \"xla\":\n","                xm.mark_step()\n","\n","            # concatenate the new generation\n","            idx = idx.index_copy(0, input_pos, idx_next)\n","\n","            # if <eos> token is triggered, return the output (stop generation)\n","            if idx_next == eos_id:\n","                return idx[:input_pos]  # include the EOS token\n","\n","        return idx\n","\n","    # LLM generation 함수\n","    def ans(self, user_message, max_new_tokens, top_k, temperature):\n","        self.history = self.history + [[user_message, None]]\n","        instruction = self.history[-1][0].strip()\n","        sample = { \"instruction\" : instruction, \"input\" : None }\n","        prompt = self.generate_prompt(sample)\n","        encoded_prompt = self.tokenizer.encode(prompt, bos=True, eos=False, device=self.fabric.device)\n","\n","        y = self.generate(\n","            idx=encoded_prompt,\n","            max_new_tokens=max_new_tokens,\n","            temperature=temperature,\n","            top_k=top_k,\n","            eos_id=self.tokenizer.eos_id\n","        )\n","\n","        self.model.reset_cache()\n","\n","        response = self.tokenizer.decode(y)\n","        response = response.split('응답:')[1].strip().replace('�', '')\n","\n","        # history 업데이트\n","        self.history[-1][1] = response\n","        return response\n","\n","def load_model():\n","    # Settings for inference\n","    # Precision setting for float32 matmul operations. It's important for some CUDA devices.\n","    torch.set_float32_matmul_precision(\"high\")\n","\n","    checkpoint_path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\")\n","    tokenizer_path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n","    quantize = None  # \"gptq.int4\" or \"llm.int8\"\n","\n","    fabric = L.Fabric(devices=1)\n","    dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n","\n","    with EmptyInitOnDevice(device=fabric.device, dtype=dtype, quantization_mode=quantize):\n","        model = LLaMA.from_name(\"7B\")\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint)\n","\n","    model.eval()\n","    model = fabric.setup_module(model)\n","\n","    tokenizer = Tokenizer(tokenizer_path)\n","\n","    return model, tokenizer, fabric\n","\n","class DataItem(BaseModel):\n","    message: str\n","\n","class CustomFastAPI(FastAPI):\n","    def __init__(self, chat_bot):\n","          super().__init__()\n","          self.input_msg = \"\"\n","          self.chat_bot = chat_bot\n","          self.add_routes()\n","\n","    def add_routes(self):\n","        @self.post(\"/post_data\")\n","        async def post_data(data: DataItem):\n","            print(\"user input:\", data.message)\n","            self.input_msg = data.message\n","            response = {'status' : 'success'}\n","            return response\n","\n","        @self.get(\"/get_data\")\n","        def get_data():\n","          ans = self.chat_bot.ans(self.input_msg, 512, 200, 0.5)\n","          ans_dict = {'generated_ans' : ans}\n","          return ans_dict\n","\n","def main():\n","    # 모델, 토크나이저 로드\n","    model, tokenizer, fabric = load_model()\n","\n","    # 챗봇 객체 생성\n","    chat_bot = ChatBot(model, tokenizer, fabric)\n","\n","    app = CustomFastAPI(chat_bot)\n","\n","    nest_asyncio.apply()  # ASGI server\n","\n","    public_url = ngrok.connect(8000)  # FastAPI server\n","    print(\"FastAPI server is available at:\", public_url)\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjgzSjxRUoL-"},"outputs":[],"source":["# evaluation\n","\n","!pip install fire\n","!pip install peft\n","\n","\"\"\"Run evaluation of medAlpaca models on the USMLE self assessment.\n","The questions can be downloaded at: https://huggingface.co/medalapca/\n","\n","Example evaluation:\n","\n","Assume you have downloaded the steps into the folder \"usmle\" you can now evaluate\n","a medalpaca model:\n","\n","```bash\n","export HF_HOME=/path/to/hf_cache\n","\n","python eval_usmle.py \\\n","    --model_name 'medalpaca/medalpaca-lora-13b-8bit' \\\n","    --prompt_template '../medalpaca/prompt_templates/medalpaca.json' \\\n","    --base_model 'decapoda-research/llama-13b-hf' \\\n","    --peft True \\\n","    --load_in_8bit True \\\n","    --path_to_exams 'data/test/'\n","\n","This will create three new files in 'data/test', named stepX_MODELNAME.json.\n","\n","The generation methods it hardcoded to the `sampling` dict, feel free to adapt this\n","\n","\"\"\"\n","!python drive/MyDrive/eval_usmle.py \\\n","    --model_name 'drive/MyDrive/model/finetuned/7B' \\\n","    --prompt_template 'drive/MyDrive/templates/alpaca_kr.json' \\\n","    --base_model 'decapoda-research/llama-7b-hf' \\\n","    --peft True \\\n","    --load_in_8bit True \\\n","    --path_to_exams 'data/test/'"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOJdyrVQhLuYwiw3WwJDX0s","gpuType":"A100","machine_shape":"hm","mount_file_id":"1NKGg6L8RnJ-Hi6jvdTCvlHCtGleP38kj","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
